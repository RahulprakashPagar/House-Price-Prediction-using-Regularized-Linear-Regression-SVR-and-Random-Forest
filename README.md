# House-Price-Prediction-using-Regularized-Linear-Regression-SVR-and-Random-Forest
A machine learning project that predicts house prices using multiple regression techniques â€” Linear Regression (with L1, L2, and Elastic Net regularization), Support Vector Regression, and Random Forest Regression â€” applied on the KC Housing dataset from Kaggle.

# ğŸ  House Price Prediction using Regularized Regression and Ensemble Models

This project applies **Supervised Machine Learning (Regression)** techniques to predict housing prices using the **KC Housing Dataset** from Kaggle.  
It explores **L1 (Lasso), L2 (Ridge), Elastic Net, Support Vector Regression (SVR)**, and **Random Forest Regression**, comparing their performance, interpretability, and overfitting behavior.

---

## ğŸ“Š Project Overview
- **Goal:** Predict house prices (`price`) based on housing features  
- **Dataset:** [KC Housing Dataset on Kaggle](https://www.kaggle.com/datasets?search=kc_)  
- **Techniques Used:** Linear Regression (L1/L2/Elastic Net), SVR, Random Forest  
- **Libraries:** pandas, sklearn, matplotlib, plotly  
- **Environment:** Google Colab / Jupyter Notebook  
- **Author:** [Rahul Pagar](https://www.linkedin.com/in/rahul-pagar1993)

---

### ğŸ·ï¸ Keywords
machine-learning, regression, linear-regression, elastic-net, ridge, lasso, support-vector-regression, random-forest, python, sklearn, data-science

---

## âš™ï¸ Workflow and Methods

### 1ï¸âƒ£ Data Preparation
- Imported dataset using `read_csv()`
- Dropped unnecessary columns (e.g., `date`)
- Checked for missing values â†’ none found
- Created correlation matrix and **heatmap** to identify multicollinearity  
- Dropped highly correlated feature (`sqft_above`)
- Standardized features using `StandardScaler()`

---

### 2ï¸âƒ£ Linear Regression Models

#### â€¢ Without Regularization
- Implemented using `SGDRegressor(penalty=None)`
- Tuned `eta0` (learning rate) and `max_iter` using GridSearchCV  
- **Best RÂ² = 0.6997** â†’ ~70% variance explained

#### â€¢ Elastic Net Regularization (L1 + L2)
- Combines Lasso and Ridge effects for balanced sparsity + shrinkage  
- Tuned `alpha`, `eta0`, `l1_ratio`, `max_iter` with GridSearchCV  
- **Optimal Params:** `alpha=0.01`, `eta0=0.001`, `l1_ratio=0.40`, `max_iter=1500`  
- **RÂ² = 0.6998**

#### â€¢ Ridge Regression (L2)
- Penalizes large coefficients, reduces overfitting  
- **RÂ² = 0.6993**

#### â€¢ Lasso Regression (L1)
- Performs feature selection by shrinking coefficients to zero  
- **RÂ² = 0.6997**

---

### 3ï¸âƒ£ Support Vector Regression (SVR)

#### â€¢ Without Regularization
- Kernels = [`linear`, `poly`, `rbf`, `sigmoid`]; Epsilon = [100, 1000, 10000]  
- **RÂ² = 0.11 (underfitting)**

#### â€¢ With Regularization (L2)
- Tuned `kernel`, `C`, `epsilon` using GridSearchCV (CV = 10)  
- **Optimal Params:** `kernel='poly'`, `C=10000`, `epsilon=1000`  
- **RÂ² = 0.76 (76%)**

---

### 4ï¸âƒ£ Random Forest Regression
- Tuned `n_estimators` and `max_features` via GridSearchCV (CV = 5)
- **Optimal Params:** `n_estimators=24`, `max_features=None`  
- **RÂ² = 0.9798 (97.98%)** â†’ Indicates possible overfitting

#### â€¢ Feature Importance
Top features:  
`grade`, `sqft_living`, `lat`, `long`, `waterfront`

After selecting important features and re-training:  
**RÂ² = 0.9761 (97.61%)**

---

## ğŸ§  Model Comparison

| Model | Regularization | RÂ² Score | Interpretability | Notes |
|-------|----------------|----------|------------------|-------|
| Linear Regression | None | 0.6997 | âœ… High | Baseline |
| Elastic Net | L1+L2 | 0.6998 | âœ… Balanced | Best among linear |
| Lasso | L1 | 0.6997 | âœ… High (sparse model) | Feature selection |
| Ridge | L2 | 0.6993 | âœ… Medium | No feature removal |
| SVR (with L2) | L2 (C = 10000) | 0.76 | âš™ï¸ Medium | Handles non-linear patterns |
| Random Forest | Tree ensemble | 0.9761 | âŒ Low | Overfitting observed |

---

## ğŸ“ˆ Visualizations
- Correlation Heatmap (Plotly)  
- Beta Coefficient Bar Charts (for L1, L2, Elastic Net)  
- Feature Importance Bar Chart (Random Forest)

---

## ğŸ“‘ Files, Author & Conclusion

### ğŸ“ Files Included
- **`Code_Rahul Pagar.py`** â€” Full Python implementation  
- **`Data File_kc_housing Data.csv`** â€” Kaggle dataset  
- **`Report_Rahul_Pagar.pdf`** â€” Comprehensive report & analysis  

---

### ğŸ‘¨â€ğŸ’» Author
**Rahul Pagar**  
ğŸ“ MSc in Business Analytics â€” Dublin Business School  
ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/rahul-pagar1993)

---

### ğŸ Conclusion
Among all models, **Random Forest** achieved the highest RÂ² (97.98%) but showed overfitting.  
**Elastic Net** and **Lasso regression** offer a better balance between accuracy, interpretability, and generalization.  
**Support Vector Regression (SVR)** performed moderately well for non-linear patterns.

# -*- coding: utf-8 -*-

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18wH8wDpoa3FZjxQv3CaS7mWKsPp1esq5

**Importing required Libraries**
"""

# Importing read_csv and DataFrame from pandas for data manipulation and reading CSV files
from pandas import read_csv, DataFrame

# Importing figure_factory from plotly for creating interactive plots
from plotly import figure_factory

# Importing StandardScaler from sklearn for standardizing features by removing the mean and scaling to unit variance
from sklearn.preprocessing import StandardScaler

# Importing linear_model from sklearn for implementing linear regression models
from sklearn import linear_model

# Importing GridSearchCV from sklearn.model_selection for hyperparameter tuning through cross-validation
from sklearn.model_selection import GridSearchCV

"""**Data Preparation**"""

df=read_csv('/content/kc_housing Data.csv') # Reading the CSV file located at '/content/kc_house_data.csv' and storing it in the dataframe 'df'

# This function is a pandas method that generates descriptive statistics
# (like count, mean, std deviation, min, max, and percentiles) of the numeric columns in the DataFrame `df`.
# This method is typically used to get a quick summary of the dataset to understand its distribution and key statistics.
df.describe()

df.info() # To get details like column names , rows, and datatypes

df.isnull().any() # To get the details for missing values

df.info()

df1 = df.drop(['date'], axis = 1) # Dropping these column as it is not adding any value during analysis.

df1.info()

# Importing necessary libraries to calculate and visualize correlations
# This code computes the correlation matrix for the dataframe df1
cor=df1.corr() # # Calculate the correlation matrix, which gives pairwise correlation between features/variables
cor

f=figure_factory.create_annotated_heatmap(cor.values,list(cor.columns),list(cor.columns),cor.round(2).values,showscale=True)
f.show() # Display the generated heatmap

df2 = df1.drop(['sqft_above'], axis = 1)

# Seggregate the dataset into dependent and independent variables
x = df2.drop(['price'],axis=1)
y= df2['price']

# Standardisation/normalisation of the dataset
x_st=StandardScaler().fit_transform(x)

x_st.shape

"""**Linear regression for price prediction**"""

# Linear Regression
LR=linear_model.SGDRegressor(random_state=1,penalty=None) # Initialize the SGDRegressor (Stochastic Gradient Descent Regressor) for linear regression without regularization.
# 'random_state=1' ensures reproducibility and 'penalty=None' means no regularization is applied to the model.
hyper_param={'eta0':[0.001,0.01,0.1,1],'max_iter':[1000,2000,3000,4000]} # # Define the hyperparameters to tune for grid search. 'eta0' is the learning rate, and 'max_iter' is the maximum number of iterations for the optimizer.
# Initialize GridSearchCV to perform hyperparameter tuning on the 'LR' model.
# It will search over the specified hyperparameters, using 5-fold cross-validation ('cv=5').
# The scoring metric is R^2, which measures the goodness of fit of the model (higher values are better).
grid_search=GridSearchCV(estimator=LR,param_grid=hyper_param,scoring='r2',cv=5)
grid_search.fit(x_st,y) # # Fit the model to the data (x_st is the feature matrix and y is the target vector).
# This will perform grid search and train the model on the best hyperparameters found.

LR.fit(x_st,y) # Fitting the Linear model without penalty
LR.score(x_st,y) # R^2 score tells how much variation in the dependent variable is explained by independent variable, i.e., 76% of variation in Price is explained by independent variables in the dataset.

# Optimal parameters
best_params= grid_search.best_params_
print(best_params)
best_result=grid_search.best_score_
print(best_result)
best_model=grid_search.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(DataFrame(zip(x.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))

LR1=linear_model.SGDRegressor(random_state=1,penalty=None,eta0= 0.001, max_iter=1000) # Fitting the Linear model with boptimised parameters

LR1.fit(x_st,y) # Fitting the Linear model without penalty and with optimal parameters.
LR1.score(x_st,y) # R^2 score tells how much variation in the dependent variable is explained by independent variable.

"""**Additing penalty in the Model**"""

# Adding the penalty in the model
LR2=linear_model.SGDRegressor(random_state=1,penalty='elasticnet') # Linear regression without the regularisation
hyper_param={'eta0':[0.0001,0.001,0.01,0.1,1],'max_iter':[1500,2500,3000,3500],'alpha':[0.001,0.01,0.1,1],'l1_ratio':[0.20,0.30,0.40,0.50,0.60]}
grid_search1=GridSearchCV(estimator=LR2,param_grid=hyper_param,scoring='r2',cv=5) # R^2 tells how the good the linear model has fitted the dataset.
grid_search1.fit(x_st,y) # Fitting the linear model

# Optimal parameters
best_params= grid_search1.best_params_
print(best_params)
best_result=grid_search1.best_score_
print(best_result)
best_model=grid_search1.best_estimator_
print('Beta_0:',best_model.intercept_)
#best_model.coef_
print(DataFrame(zip(x.columns,best_model.coef_),columns=['Columns/features','Beta coefficients']))

LR3=linear_model.SGDRegressor(random_state=1,penalty='elasticnet',alpha= 0.01, eta0= 0.0001, l1_ratio= 0.40, max_iter=1500) # Fitting the Linear model with boptimised parameters

LR3.fit(x_st,y)# Fitting the Linear model with penalty elasticnet and using the bestparameters from GridsearchCV
print(LR3.score(x_st,y)) # R^2 score tells how much variation in the dependent variable is explained by independent variable, i.e.69.96% of variation in Price is explained by independent variables in the dataset.
print('Beta_0:',best_model.intercept_)
#best_model.coef_
df=print(DataFrame(zip(x.columns,best_model.coef_),columns=['features','Beta coefficients']))

import pandas as pd
import matplotlib.pyplot as plt
data_elastic_net = {
    "Features": [
        "id", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors",
        "waterfront", "view", "condition", "grade", "sqft_basement",
        "yr_built", "yr_renovated", "zipcode", "lat", "long",
        "sqft_living15", "sqft_lot15"
    ],
    "Beta Coefficients": [
        -4829.757228, -32854.746442, 31106.163748, 164994.847510,7538.948099,
        2937.654931,50304.094599,41169.247669,16848.626448,112403.799110,-13709.332129,-76366.426857,7795.859177,
        -31659.339420,82877.677404,-29951.303389,13920.283970,-10145.816870

    ]
}

zd1=pd.DataFrame(data_elastic_net)
# Plot bar chart
plt.figure(figsize=(8, 6))
plt.bar(zd1["Features"], zd1["Beta Coefficients"], color="skyblue")
plt.title("Beta Coefficients for Features with elastic net", fontsize=16)
plt.xlabel("Features", fontsize=12)
plt.ylabel("Beta Coefficients", fontsize=12)
plt.xticks(rotation=45, ha="right", fontsize=10)
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Linear regression with Lasso

"""**Linear regression with lasso**"""

# Adding the penalty in the model with penalty = 'l1'
LR_l=linear_model.SGDRegressor(random_state=1,penalty='l1') # Linear regression without the regularisation
hyper_param_l={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000],'alpha':[0.001,0.01,0.1,1],'l1_ratio': [1]}
grid_search_l=GridSearchCV(estimator=LR_l,param_grid=hyper_param_l,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search_l.fit(x_st,y) # Fitting the linear model

# Optimal parameters
best_params= grid_search_l.best_params_
print(best_params)
best_result=grid_search_l.best_score_
print(best_result)
best_model=grid_search_l.best_estimator_
print('Beta_0:',best_model.intercept_)

LR_lasso=linear_model.SGDRegressor(random_state=1,penalty='elasticnet',alpha= 0.001, eta0= 0.003, l1_ratio= 1, max_iter=1000) # Fitting the Linear model with boptimised parameters

LR_lasso.fit(x_st,y) # Fitting the Linear model with penalty l1 and using the bestparameters from GridsearchCV

LR_lasso.score(x_st,y)

LR_lasso.coef_
print(DataFrame(zip(x.columns,LR_lasso.coef_),columns=['Columns/features','Beta coefficients']))

data_lasso = {
    "Features": [
        "id", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors",
        "waterfront", "view", "condition", "grade", "sqft_basement",
        "yr_built", "yr_renovated", "zipcode", "lat", "long",
        "sqft_living15", "sqft_lot15"
    ],
    "Beta Coefficients": [
        -4829.757228, -32854.746442, 31106.163748, 164994.847510,7538.948099,
        2937.654931,50304.094599,41169.247669,16848.626448,112403.799110,-13709.332129,-76366.426857,7795.859177,
        -31659.339420,82877.677404,-29951.303389,13920.283970,-10145.816870

    ]
}
zd2=pd.DataFrame(data_lasso)
# Plot bar chart
plt.figure(figsize=(8, 6))
plt.bar(zd2["Features"], zd2["Beta Coefficients"], color="skyblue")
plt.title("Beta Coefficients for Features with L1", fontsize=16)
plt.xlabel("Features", fontsize=12)
plt.ylabel("Beta Coefficients", fontsize=12)
plt.xticks(rotation=45, ha="right", fontsize=10)
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""**Impact of L2 on linear regression**"""

# Adding the penalty in the model with penalty = 'l2'
LR_r=linear_model.SGDRegressor(random_state=1,penalty='l2') # Linear regression without the regularisation
hyper_param2={'eta0':[0.005,0.003,0.04,0.02,0.25,0.1,1],'max_iter':[1000,1100,1300,1500,1800,2000],'alpha':[0.001,0.01,0.1,1],'l1_ratio': [0]}
grid_search2=GridSearchCV(estimator=LR3,param_grid=hyper_param2,scoring='r2',cv=7) # R^2 tells how the good the linear model has fitted the dataset.
grid_search2.fit(x_st,y) # Fitting the linear model

# Optimal parameters
best_params= grid_search2.best_params_
print(best_params)
best_result=grid_search2.best_score_
print(best_result)
best_model=grid_search2.best_estimator_
print('Beta_0:',best_model.intercept_)

LR_ridge=linear_model.SGDRegressor(random_state=1,penalty='elasticnet',alpha= 0.01, eta0= 0.005, l1_ratio=0, max_iter=1000) # Fitting the Linear model with boptimised parameters

LR_ridge.fit(x_st,y) # Fitting the Linear model with penalty l1 and using the bestparameters from GridsearchCV

LR_ridge.score(x_st,y)

LR_ridge.coef_
print(DataFrame(zip(x.columns,LR_ridge.coef_),columns=['Columns/features','Beta coefficients']))

data_ridge = {
    "Features": [
        "id", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors",
        "waterfront", "view", "condition", "grade", "sqft_basement",
        "yr_built", "yr_renovated", "zipcode", "lat", "long",
        "sqft_living15", "sqft_lot15"
    ],
    "Beta Coefficients": [
        -5269.940326,-29930.70548,31382.272570,156432.007861, 8760.905234,3617.526649,49769.842025,
        41158.500491,16638.996780,111729.699071,-12175.771762,-74241.038075,7842.432336,
        -31238.752134,81733.416184,-29142.797673,16503.971523,-10342.662456

    ]
}
zd3=pd.DataFrame(data_ridge)
# Plot bar chart
plt.figure(figsize=(8, 6))
plt.bar(zd3["Features"], zd3["Beta Coefficients"], color="skyblue")
plt.title("Beta Coefficients for Features with L2", fontsize=16)
plt.xlabel("Features", fontsize=12)
plt.ylabel("Beta Coefficients", fontsize=12)
plt.xticks(rotation=45, ha="right", fontsize=10)
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

input=x[:10]

input_data_scaled = StandardScaler().fit_transform(input)
predictions = LR_ridge.predict(input_data_scaled)
print(DataFrame(predictions))

"""**Support vector regression**"""

# method 1:Support vector regression without L2 regularization

from sklearn.svm import SVR
SVRegressor1 = SVR()
cv=10
Hparameters1= {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'epsilon': [100,1000,10000]}
grid_search2 = GridSearchCV(estimator=SVRegressor1, param_grid=Hparameters1, scoring='r2')
grid_search2.fit(x_st, y)

best_parameters = grid_search2.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search2.best_score_
print("Best result: ", best_result)

svr_regressor2= SVR(kernel='linear',epsilon=100)
svr_regressor2.fit(x_st, y)
svr_regressor2.score(x_st, y)

svr_regressor2.coef_
DataFrame(zip(x.columns,svr_regressor2.coef_),columns=['Columns/features','Beta coefficients'])

#Method 2: Impact of L2 regularization on support vecor regression.

from sklearn.svm import SVR
SVRegressor3= SVR()
cv=10
Hparameters2= {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [100,1000,10000], 'epsilon': [100,1000,10000]}
grid_search3 = GridSearchCV(estimator=SVRegressor3, param_grid=Hparameters2, scoring='r2')
grid_search3.fit(x_st, y)

best_parameters = grid_search3.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search3.best_score_
print("Best result: ", best_result)

svr_regressor= SVR(kernel='poly', C=10000, epsilon=1000)
svr_regressor.fit(x_st, y)

svr_regressor.score(x_st, y)

"""**Random Forest Regression**"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
from pandas import Series

from sklearn.ensemble import RandomForestRegressor
RF_Regressor1 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [5,10,15,20,22,24], 'max_features':['sqrt','log2', None]}
grid_search4 = GridSearchCV(estimator=RF_Regressor1, param_grid=no_Trees, scoring='r2', cv=5)
grid_search4.fit(x_st, y)

best_parameters = grid_search4.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search4.best_score_
print("best_score: ", best_result)

from sklearn import ensemble

# Refit the random forest on the optimal n_estimators parameter
RF2_regressor=ensemble.RandomForestRegressor(n_estimators=24,criterion='squared_error',max_features=None,random_state=1)
RF2_regressor.fit(x_st,y)

best_result = RF2_regressor.score(x_st,y)
print("r2: ", best_result)

"""**Random Forest With Important features selection**"""

from sklearn.ensemble import RandomForestRegressor
RF_Regressor2 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [2,4,6,8,10,12,14], 'max_features':['sqrt','log2', None]}
grid_search5 = GridSearchCV(estimator=RF_Regressor2, param_grid=no_Trees, scoring='r2', cv=5)
grid_search5.fit(x_st, y)

best_parameters = grid_search5.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search5.best_score_
print("best_score: ", best_result)
Important_feature = Series(grid_search5.best_estimator_.feature_importances_, index=list(x)).sort_values(ascending=False) # Getting feature importances list for the best model
print(Important_feature)

import matplotlib.pyplot as plt

# Feature importance data
features = [
    "grade", "sqft_living", "lat", "long", "waterfront",
    "sqft_living15", "yr_built", "zipcode", "sqft_lot15",
    "sqft_lot", "view", "id", "bathrooms", "sqft_basement",
    "bedrooms", "condition", "floors", "yr_renovated"
]

importance = [
    0.356690, 0.240884, 0.159495, 0.070393, 0.029953,
    0.029486, 0.028544, 0.015222, 0.014145, 0.013511,
    0.009189, 0.008678, 0.007990, 0.005992, 0.003317,
    0.002835, 0.002029, 0.001647
]

# Sorting features by importance
features_sorted, importance_sorted = zip(*sorted(zip(features, importance), key=lambda x: x[1], reverse=True))

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(features_sorted, importance_sorted, color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.xlabel("Features")
plt.ylabel("Importance")
plt.title("Feature Importance in Random Forest")
plt.tight_layout()
plt.show()

# Selecting features with higher sifnificance and redefining feature set
z = df2[['grade', 'sqft_living', 'lat', 'long','waterfront']]
z1_ = StandardScaler().fit_transform(z)

RF_Regressor3 = RandomForestRegressor(criterion='squared_error', max_features='sqrt', random_state=1)
no_Trees = {'n_estimators': [2,4,6,8,10,12,14], 'max_features':['sqrt','log2', None]}
grid_search6 = GridSearchCV(estimator=RF_Regressor3, param_grid=no_Trees, scoring='r2', cv=5)
grid_search6.fit(z1_, y)

best_parameters = grid_search6.best_params_
print("Best parameters: ", best_parameters)
best_result = grid_search5.best_score_

# Refit the random forest on the optimal n_estimators parameter
RF_regressor_feature=ensemble.RandomForestRegressor(n_estimators=12,criterion='squared_error',max_features=None,random_state=1)
RF_regressor_feature.fit(z1_, y)

best_result = RF_regressor_feature.score(z1_, y)
print("r2: ", best_result)

"""**Price prediction using new Data**"""

# Prediction with New Data
import pandas as pd
new_data = pd.DataFrame({
    'id': [7129300524],
    'bedrooms': [5],
    'bathrooms': [3.5],
    'sqft_living': [1680],
    'sqft_lot':[9234],
    'floors': [2.1],
    'waterfront': [0],
    'view': [2],
    'condition':[5],
    'grade': [13],
    'sqft_basement':[1870],
    'yr_built':[2001],
    'yr_renovated':[2018],
    'zipcode':[98742],
    'lat':[47.9234],
    'long':[-122.343],
    'sqft_living15':[1721],
    'sqft_lot15':[4394]
})

# Get the original column names from before the transformation
original_columns = ['id', 'bedrooms', 'bathrooms', 'sqft_living' , 'floors','waterfront','view','condition','grade','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15','sqft_lot15']

for col in original_columns:
    if col not in new_data.columns:
        new_data[col]

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(new_data)
new_data_scaled = scaler.transform(new_data)

"""**Prediction using Regularized SVR **"""

# Prediction
prediction = svr_regressor.predict(new_data_scaled)  # Assuming RF_regressor_feature is the intended model
print("Prediction for New Data With Regularized Support Vector Regressor:", prediction)

"""**Prediction using Elastic Net**"""

predict1=LR3.predict(new_data_scaled)
print("Prediction for New Data With Elastic net:", predict1)

"""**Prediction using Lasso**"""

predict2=LR_lasso.predict(new_data_scaled)
print("Prediction for New Data With Lasso:", predict2)